name: $(Date:yyyyMMdd)$(Rev:.r)-Addons
trigger: none
pr: none

parameters:
  - name: env
    displayName: Environment
    type: string
    values: [nonprod, prod]

variables:
  TF_DIR: 'terraform'
  TF_VAR_FILE: '${{ parameters.env }}.tfvars'

  AWS_SERVICE_CONNECTION: 'yousif-aws-v2'
  AWS_REGION: 'ap-northeast-2'

  TF_BACKEND_BUCKET: 'yousif-project-bucket'
  TF_BACKEND_KEY: '${{ parameters.env }}/terraform.tfstate'

pool:
  name: Yousif-Agent_Pool

jobs:
- job: Addons
  displayName: 'Addons Deployment (Helm, Vault, Argo, Sonar, Nginx)'
  timeoutInMinutes: 160

  steps:
  - checkout: self

  - task: TerraformInstaller@1
    displayName: Install Terraform
    inputs:
      terraformVersion: latest

  - task: AWSShellScript@1
    displayName: Terraform init (S3 backend + plugin cache)
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"

        echo "Terraform version:"
        terraform -version

        export TF_PLUGIN_CACHE_DIR="${TF_PLUGIN_CACHE_DIR:-$HOME/.terraform.d/plugin-cache}"
        mkdir -p "$TF_PLUGIN_CACHE_DIR" || true
        echo "TF_PLUGIN_CACHE_DIR=$TF_PLUGIN_CACHE_DIR"

        if ! command -v aws >/dev/null 2>&1; then
          echo "##[error]aws CLI not found on agent. Install AWS CLI on the agent."
          exit 1
        fi

        echo "==> terraform init (bucket=$(TF_BACKEND_BUCKET) key=$(TF_BACKEND_KEY))"
        terraform init -reconfigure \
          -backend-config="bucket=$(TF_BACKEND_BUCKET)" \
          -backend-config="key=$(TF_BACKEND_KEY)" \
          -backend-config="region=$(AWS_REGION)" \
          -backend-config="encrypt=true"

        terraform output || true

  - task: AWSShellScript@1
    displayName: Configure kubeconfig
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"

        CLUSTER_NAME="$(terraform output -raw cluster_name 2>/dev/null || true)"

        if [ -z "$CLUSTER_NAME" ]; then
          echo "##[error]cluster_name output is empty. Wrong backend key or no outputs."
          echo "Backend key: $(TF_BACKEND_KEY)"
          terraform output || true
          exit 1
        fi

        if [ ${#CLUSTER_NAME} -gt 100 ] || echo "$CLUSTER_NAME" | grep -q '[[:space:]]'; then
          echo "##[error]Invalid cluster_name read from terraform output:"
          echo "$CLUSTER_NAME"
          terraform output || true
          exit 1
        fi

        aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$(AWS_REGION)"
        kubectl get nodes --request-timeout=30s

  - task: HelmInstaller@1
    displayName: Install Helm
    inputs:
      helmVersionToInstall: 'latest'

  - task: AWSShellScript@1
    displayName: Install Nginx Ingress
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail

        handle_error() {
          echo "##[error] Nginx Ingress deployment failed!"
          kubectl get all -n ingress-nginx || true
          kubectl describe pod -l app.kubernetes.io/name=ingress-nginx -n ingress-nginx || true
          kubectl logs -l app.kubernetes.io/name=ingress-nginx -n ingress-nginx --all-containers --tail=150 || true
          kubectl get events -n ingress-nginx --sort-by='.lastTimestamp' | tail -n 80 || true
        }
        trap 'handle_error' ERR

        helm upgrade --install ingress-nginx ingress-nginx \
          --repo https://kubernetes.github.io/ingress-nginx \
          --namespace ingress-nginx --create-namespace \
          --version 4.11.3 \
          --set controller.service.type=LoadBalancer \
          --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"="nlb" \
          --set controller.admissionWebhooks.enabled=false \
          --wait --timeout 25m --rollback-on-failure --debug

        kubectl get svc -n ingress-nginx ingress-nginx-controller -o wide --request-timeout=30s

  - task: AWSShellScript@1
    displayName: Link API Gateway to Load Balancer
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"

        echo "Waiting for NLB hostname..."
        NLB_DNS=""
        for i in {1..36}; do
          NLB_DNS="$(kubectl get svc -n ingress-nginx ingress-nginx-controller \
            -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' \
            --request-timeout=30s 2>/dev/null || true)"
          if [ -n "$NLB_DNS" ]; then
            echo "âœ… Found NLB DNS: $NLB_DNS"
            break
          fi
          sleep 10
        done

        if [ -z "$NLB_DNS" ]; then
          echo "##[warning]Load Balancer DNS not found. Skipping API Gateway update."
          exit 0
        fi

        LB_ARN="$(aws elbv2 describe-load-balancers \
          --query "LoadBalancers[?DNSName=='$NLB_DNS'].LoadBalancerArn | [0]" \
          --output text)"

        if [ -z "$LB_ARN" ] || [ "$LB_ARN" = "None" ]; then
          echo "##[error]Could not resolve LB ARN for DNSName=$NLB_DNS"
          exit 1
        fi

        LISTENER_ARN="$(aws elbv2 describe-listeners \
          --load-balancer-arn "$LB_ARN" \
          --query 'Listeners[0].ListenerArn' \
          --output text)"

        if [ -z "$LISTENER_ARN" ] || [ "$LISTENER_ARN" = "None" ]; then
          echo "##[error]Could not resolve Listener ARN for LB_ARN=$LB_ARN"
          exit 1
        fi

        terraform apply -auto-approve -var-file="$(TF_VAR_FILE)" \
          -var="nlb_dns_name=http://$NLB_DNS" \
          -var="nlb_listener_arn=$LISTENER_ARN"

  - task: AWSShellScript@1
    displayName: Export Terraform Outputs (Cognito/APIGW)
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"

        COGNITO_CLIENT_ID="$(terraform output -raw cognito_app_client_id 2>/dev/null || true)"
        COGNITO_ISSUER_URL="$(terraform output -raw cognito_issuer_url 2>/dev/null || true)"
        APIGW_URL="$(terraform output -raw api_gateway_endpoint 2>/dev/null || true)"
        USER_POOL_ID="$(terraform output -raw cognito_user_pool_id 2>/dev/null || true)"

        if [ -z "$COGNITO_CLIENT_ID" ] || [ -z "$COGNITO_ISSUER_URL" ] || [ -z "$APIGW_URL" ] || [ -z "$USER_POOL_ID" ]; then
          echo "##[error]Missing required terraform outputs."
          terraform output || true
          exit 1
        fi

        COGNITO_CLIENT_SECRET="$(aws cognito-idp describe-user-pool-client \
          --user-pool-id "$USER_POOL_ID" \
          --client-id "$COGNITO_CLIENT_ID" \
          --query 'UserPoolClient.ClientSecret' --output text 2>/dev/null || true)"

        COOKIE_SECRET="STABLE_SECRET_V1_FOR_32_BYTE_AES"

        if [ -z "$COGNITO_CLIENT_SECRET" ] || [ "$COGNITO_CLIENT_SECRET" = "None" ]; then
          echo "##[warning]Cognito client secret not available (public client)."
          COGNITO_CLIENT_SECRET=""
        fi

        echo "##vso[task.setvariable variable=COGNITO_CLIENT_ID;isSecret=true]$COGNITO_CLIENT_ID"
        echo "##vso[task.setvariable variable=COGNITO_CLIENT_SECRET;isSecret=true]$COGNITO_CLIENT_SECRET"
        echo "##vso[task.setvariable variable=COGNITO_ISSUER_URL]$COGNITO_ISSUER_URL"
        echo "##vso[task.setvariable variable=APIGW_URL]$APIGW_URL"
        echo "##vso[task.setvariable variable=COOKIE_SECRET;isSecret=true]$COOKIE_SECRET"

  - task: AWSShellScript@1
    displayName: Deploy OAuth2-Proxy (debug-friendly)
    env:
      COGNITO_CLIENT_ID: $(COGNITO_CLIENT_ID)
      COGNITO_CLIENT_SECRET: $(COGNITO_CLIENT_SECRET)
      COOKIE_SECRET: $(COOKIE_SECRET)
      COGNITO_ISSUER_URL: $(COGNITO_ISSUER_URL)
      APIGW_URL: $(APIGW_URL)
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        NS="ingress-nginx"
        REL="oauth2-proxy"

        HELM_BASE="helm upgrade --install ${REL} oauth2-proxy \
          --repo https://oauth2-proxy.github.io/manifests \
          --namespace ${NS} --create-namespace \
          --wait --timeout 30m \
          --set config.clientID=\"$COGNITO_CLIENT_ID\" \
          --set config.cookieSecret=\"$COOKIE_SECRET\" \
          --set extraArgs.provider=\"oidc\" \
          --set extraArgs.oidc-issuer-url=\"$COGNITO_ISSUER_URL\" \
          --set extraArgs.oidc-extra-audience=\"$COGNITO_CLIENT_ID\" \
          --set extraArgs.skip-provider-button=true \
          --set extraArgs.upstream=\"file:///dev/null\" \
          --set extraArgs.http-address=\"0.0.0.0:4180\" \
          --set extraArgs.redirect-url=\"${APIGW_URL}/oauth2/callback\" \
          --set ingress.enabled=true \
          --set ingress.className=\"nginx\" \
          --set ingress.path=\"/oauth2\" \
          --set ingress.hosts[0]=\"\""

        set +e
        if [ -n "${COGNITO_CLIENT_SECRET:-}" ]; then
          eval "$HELM_BASE --set config.clientSecret=\"$COGNITO_CLIENT_SECRET\""
          RC=$?
        else
          eval "$HELM_BASE"
          RC=$?
        fi
        set -e

        kubectl get pods -n "${NS}" -l "app.kubernetes.io/instance=${REL}" -o wide || true
        kubectl logs -n "${NS}" -l "app.kubernetes.io/instance=${REL}" --tail=200 --all-containers || true
        kubectl describe pods -n "${NS}" -l "app.kubernetes.io/instance=${REL}" || true
        kubectl get events -n "${NS}" --sort-by=.lastTimestamp | tail -n 60 || true

        if [ $RC -ne 0 ]; then
          echo "##[error]oauth2-proxy helm deploy failed (rc=$RC)."
          exit $RC
        fi

  - task: AWSShellScript@1
    displayName: Cleanup Vault Webhooks
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        kubectl delete MutatingWebhookConfiguration vault-agent-injector-cfg --ignore-not-found=true || true

  - task: AWSShellScript@1
    displayName: Install Vault
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cat > vault-ingress-values.yaml <<'EOF'
        server:
          ha:
            enabled: false
            replicas: 1
          dataStorage:
            storageClass: gp2
            size: 1Gi
          service:
            type: ClusterIP
          ingress:
            enabled: true
            ingressClassName: nginx
            hosts:
              - host: ""
                paths:
                  - /vault/?(.*)
                  - /(ui(?:/.*)?)
            annotations:
              nginx.ingress.kubernetes.io/use-regex: "true"
              nginx.ingress.kubernetes.io/rewrite-target: /\$1
        ui:
          enabled: true
        EOF

        helm upgrade --install vault vault \
          --repo https://helm.releases.hashicorp.com \
          --namespace yousif --create-namespace \
          --wait --timeout 15m --rollback-on-failure \
          -f vault-ingress-values.yaml

  - task: AWSShellScript@1
    displayName: Install Argo CD
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cat > argocd-ingress-values.yaml <<'EOF'
        redis-ha:
          enabled: false
        controller:
          replicas: 1
        server:
          replicas: 1
          service:
            type: ClusterIP
          ingress:
            enabled: false
          extraArgs:
            - --insecure
            - --rootpath=/argocd
        repoServer:
          replicas: 1
        applicationController:
          replicas: 1
        EOF

        helm upgrade --install argocd argo-cd \
          --repo https://argoproj.github.io/argo-helm \
          --namespace yousif --create-namespace \
          --wait --timeout 20m --rollback-on-failure \
          -f argocd-ingress-values.yaml

        kubectl apply -f - <<'INGRESS_EOF'
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        metadata:
          name: argocd-server
          namespace: yousif
          annotations:
            nginx.ingress.kubernetes.io/backend-protocol: HTTP
        spec:
          ingressClassName: nginx
          rules:
          - http:
              paths:
              - path: /argocd
                pathType: Prefix
                backend:
                  service:
                    name: argocd-server
                    port:
                      number: 80
        INGRESS_EOF

  - task: AWSShellScript@1
    displayName: Install SonarQube
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cat > sonarqube-values.yaml <<'EOF'
        service:
          type: ClusterIP
        ingress:
          enabled: true
          ingressClassName: nginx
          hosts:
            - name: ""
              path: "/sonarqube"
          annotations:
            nginx.ingress.kubernetes.io/proxy-body-size: "64m"
        sonarWebContext: /sonarqube
        EOF

        helm upgrade --install sonarqube sonarqube \
          --repo https://SonarSource.github.io/helm-chart-sonarqube \
          --namespace yousif --create-namespace \
          --wait --timeout 30m --rollback-on-failure \
          -f sonarqube-values.yaml