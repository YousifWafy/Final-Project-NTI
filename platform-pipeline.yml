name: $(Date:yyyyMMdd)$(Rev:.r)-Platform
trigger: none
pr: none

parameters:
  - name: env
    displayName: Environment
    type: string
    values: [nonprod, prod]

  - name: action
    displayName: Action
    type: string
    default: deploy
    values: [deploy, destroy]

variables:
  TF_DIR: 'terraform'
  TF_VAR_FILE: '${{ parameters.env }}.tfvars'

  AWS_SERVICE_CONNECTION: 'yousif-aws-v2'
  AWS_REGION: 'ap-northeast-2'

  # IMPORTANT: use the same backend as infra pipeline
  TF_BACKEND_BUCKET: 'yousif-project-bucket'
  TF_BACKEND_KEY: '${{ parameters.env }}/terraform.tfstate'

pool:
  name: Yousif-Agent_Pool

jobs:
- job: Platform
  displayName: 'Platform Addons (Nginx, OAuth2-Proxy, Vault, ArgoCD, SonarQube)'
  timeoutInMinutes: 200

  steps:
  - checkout: self
    fetchDepth: 1

  # -----------------------------
  # 0) Tools
  # -----------------------------
  - task: TerraformInstaller@1
    displayName: Install Terraform
    inputs:
      terraformVersion: latest

  - task: HelmInstaller@1
    displayName: Install Helm
    inputs:
      helmVersionToInstall: 'latest'

  # -----------------------------
  # 1) Terraform init (NO PROMPTS) + plugin cache
  # -----------------------------
  - task: AWSShellScript@1
    displayName: Terraform init (S3 backend + cache)
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"

        echo "Terraform version:"
        terraform -version

        # Use plugin cache if exists on self-hosted agent
        if [ -f "$HOME/.terraformrc" ]; then
          export TF_CLI_CONFIG_FILE="$HOME/.terraformrc"
          echo "Using TF_CLI_CONFIG_FILE=$TF_CLI_CONFIG_FILE"
        else
          echo "##[warning]~/.terraformrc not found. Providers may need internet access."
        fi

        export TF_PLUGIN_CACHE_DIR="${TF_PLUGIN_CACHE_DIR:-$HOME/.terraform.d/plugin-cache}"
        mkdir -p "$TF_PLUGIN_CACHE_DIR" || true
        echo "TF_PLUGIN_CACHE_DIR=$TF_PLUGIN_CACHE_DIR"

        echo "==> terraform init (backend-config) bucket=$(TF_BACKEND_BUCKET) key=$(TF_BACKEND_KEY)"
        terraform init -reconfigure -lockfile=readonly \
          -backend-config="bucket=$(TF_BACKEND_BUCKET)" \
          -backend-config="key=$(TF_BACKEND_KEY)" \
          -backend-config="region=$(AWS_REGION)" \
          -backend-config="encrypt=true"

        terraform output || true

  # -----------------------------
  # 2) kubeconfig from TF output
  # -----------------------------
  - task: AWSShellScript@1
    displayName: Configure kubeconfig
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"

        CLUSTER_NAME="$(terraform output -raw cluster_name 2>/dev/null || true)"
        if [ -z "$CLUSTER_NAME" ]; then
          echo "##[error]Could not read terraform output 'cluster_name'."
          terraform output || true
          exit 1
        fi

        echo "Updating kubeconfig for: $CLUSTER_NAME"
        aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$(AWS_REGION)"

        # Patch kubeconfig to use absolute aws path (macOS sed differs)
        AWS_BIN="$(command -v aws || true)"
        if [ -n "$AWS_BIN" ]; then
          if sed --version >/dev/null 2>&1; then
            sed -i "s|command: aws|command: $AWS_BIN|g" "$HOME/.kube/config" || true
          else
            sed -i '' "s|command: aws|command: $AWS_BIN|g" "$HOME/.kube/config" || true
          fi
        fi

        echo "Cluster access check:"
        kubectl get nodes --request-timeout=30s

  # -----------------------------
  # DEPLOY MODE
  # -----------------------------

  - task: AWSShellScript@1
    displayName: Deploy Nginx Ingress (NLB)
    condition: and(succeeded(), eq('${{ parameters.action }}', 'deploy'))
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail

        echo "Installing ingress-nginx (retry up to 3)..."
        for n in 1 2 3; do
          if helm upgrade --install ingress-nginx ingress-nginx \
            --repo https://kubernetes.github.io/ingress-nginx \
            --namespace ingress-nginx --create-namespace \
            --version 4.11.3 \
            --set controller.service.type=LoadBalancer \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"="nlb" \
            --set controller.admissionWebhooks.enabled=false \
            --wait --timeout 25m --atomic --debug; then
            break
          fi
          echo "Attempt $n failed. Sleeping 15s then retry..."
          sleep 15
        done

        kubectl get svc -n ingress-nginx ingress-nginx-controller -o wide --request-timeout=30s

  - task: AWSShellScript@1
    displayName: Update Terraform with NLB DNS + Listener ARN (optional)
    condition: and(succeeded(), eq('${{ parameters.action }}', 'deploy'))
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail

        echo "Waiting for NLB hostname from ingress-nginx service..."
        NLB_DNS=""

        # 36 * 10s = 6 minutes max
        for i in {1..36}; do
          NLB_DNS="$(kubectl get svc -n ingress-nginx ingress-nginx-controller \
            -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' \
            --request-timeout=30s 2>/dev/null || true)"

          if [ -n "$NLB_DNS" ]; then
            echo "âœ… Found NLB DNS: $NLB_DNS (after ${i} tries)"
            break
          fi

          echo "Attempt ${i}/36: NLB DNS not ready yet... sleeping 10s"
          sleep 10
        done

        if [ -z "$NLB_DNS" ]; then
          echo "##[warning]NLB DNS not ready after 6 minutes. Skipping terraform apply for NLB vars."
          kubectl get svc -n ingress-nginx ingress-nginx-controller -o wide || true
          exit 0
        fi

        echo "Resolving LoadBalancerArn by DNSName..."
        LB_ARN="$(aws elbv2 describe-load-balancers \
          --query "LoadBalancers[?DNSName=='$NLB_DNS'].LoadBalancerArn | [0]" \
          --output text)"

        if [ -z "$LB_ARN" ] || [ "$LB_ARN" = "None" ]; then
          echo "##[error]Could not resolve LoadBalancerArn from DNSName=$NLB_DNS"
          aws elbv2 describe-load-balancers --query 'LoadBalancers[].{DNS:DNSName,Arn:LoadBalancerArn}' --output table | head -n 200 || true
          exit 1
        fi

        echo "LB_ARN: $LB_ARN"
        echo "Resolving ListenerArn..."
        LISTENER_ARN="$(aws elbv2 describe-listeners \
          --load-balancer-arn "$LB_ARN" \
          --query 'Listeners[0].ListenerArn' \
          --output text)"

        if [ -z "$LISTENER_ARN" ] || [ "$LISTENER_ARN" = "None" ]; then
          echo "##[error]Could not resolve ListenerArn for LB_ARN=$LB_ARN"
          aws elbv2 describe-listeners --load-balancer-arn "$LB_ARN" --output table || true
          exit 1
        fi

        echo "LISTENER_ARN: $LISTENER_ARN"

        cd "$(TF_DIR)"

        # Use plugin cache if exists
        if [ -f "$HOME/.terraformrc" ]; then
          export TF_CLI_CONFIG_FILE="$HOME/.terraformrc"
        fi

        terraform apply -auto-approve \
          -var-file="$(TF_VAR_FILE)" \
          -var="nlb_dns_name=http://$NLB_DNS" \
          -var="nlb_listener_arn=$LISTENER_ARN"

  - task: AWSShellScript@1
    displayName: Export Cognito + APIGW outputs (and fetch Cognito secret)
    condition: and(succeeded(), eq('${{ parameters.action }}', 'deploy'))
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"

        # MATCH YOUR REAL OUTPUT NAMES
        COGNITO_CLIENT_ID="$(terraform output -raw cognito_app_client_id 2>/dev/null || true)"
        COGNITO_ISSUER_URL="$(terraform output -raw cognito_issuer_url 2>/dev/null || true)"
        APIGW_URL="$(terraform output -raw api_gateway_endpoint 2>/dev/null || true)"
        USER_POOL_ID="$(terraform output -raw cognito_user_pool_id 2>/dev/null || true)"

        if [ -z "$COGNITO_CLIENT_ID" ] || [ -z "$COGNITO_ISSUER_URL" ] || [ -z "$APIGW_URL" ] || [ -z "$USER_POOL_ID" ]; then
          echo "##[error]Missing required terraform outputs. Found:"
          terraform output || true
          exit 1
        fi

        # Fetch the client secret from AWS (requires the app client to have GenerateSecret enabled)
        COGNITO_CLIENT_SECRET="$(aws cognito-idp describe-user-pool-client \
          --user-pool-id "$USER_POOL_ID" \
          --client-id "$COGNITO_CLIENT_ID" \
          --query 'UserPoolClient.ClientSecret' --output text 2>/dev/null || true)"

        if [ -z "$COGNITO_CLIENT_SECRET" ] || [ "$COGNITO_CLIENT_SECRET" = "None" ]; then
          echo "##[error]Could not fetch Cognito client secret. Ensure the app client has GenerateSecret enabled."
          exit 1
        fi

        COOKIE_SECRET="STABLE_SECRET_V1_FOR_32_BYTE_AES"

        echo "##vso[task.setvariable variable=COGNITO_CLIENT_ID;isSecret=true]$COGNITO_CLIENT_ID"
        echo "##vso[task.setvariable variable=COGNITO_CLIENT_SECRET;isSecret=true]$COGNITO_CLIENT_SECRET"
        echo "##vso[task.setvariable variable=COGNITO_ISSUER_URL]$COGNITO_ISSUER_URL"
        echo "##vso[task.setvariable variable=APIGW_URL]$APIGW_URL"
        echo "##vso[task.setvariable variable=COOKIE_SECRET;isSecret=true]$COOKIE_SECRET"

  - task: AWSShellScript@1
    displayName: Deploy OAuth2-Proxy
    condition: and(succeeded(), eq('${{ parameters.action }}', 'deploy'))
    env:
      COGNITO_CLIENT_ID: $(COGNITO_CLIENT_ID)
      COGNITO_CLIENT_SECRET: $(COGNITO_CLIENT_SECRET)
      COOKIE_SECRET: $(COOKIE_SECRET)
      COGNITO_ISSUER_URL: $(COGNITO_ISSUER_URL)
      APIGW_URL: $(APIGW_URL)
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail

        helm upgrade --install oauth2-proxy oauth2-proxy \
          --repo https://oauth2-proxy.github.io/manifests \
          --namespace ingress-nginx --create-namespace \
          --wait --timeout 20m --atomic \
          --set config.clientID="$COGNITO_CLIENT_ID" \
          --set config.clientSecret="$COGNITO_CLIENT_SECRET" \
          --set config.cookieSecret="$COOKIE_SECRET" \
          --set extraArgs.provider="oidc" \
          --set extraArgs.oidc-issuer-url="$COGNITO_ISSUER_URL" \
          --set extraArgs.skip-provider-button=true \
          --set extraArgs.upstream="file:///dev/null" \
          --set extraArgs.http-address="0.0.0.0:4180" \
          --set extraArgs.redirect-url="${APIGW_URL}/oauth2/callback" \
          --set ingress.enabled=true \
          --set ingress.className="nginx" \
          --set ingress.path="/oauth2" \
          --set ingress.hosts[0]=""

  - task: AWSShellScript@1
    displayName: Deploy Vault
    condition: and(succeeded(), eq('${{ parameters.action }}', 'deploy'))
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        kubectl delete MutatingWebhookConfiguration vault-agent-injector-cfg --ignore-not-found=true || true

        cat > vault-values.yaml <<'EOF'
        server:
          ha:
            enabled: false
            replicas: 1
          dataStorage:
            storageClass: gp2
            size: 1Gi
          service:
            type: ClusterIP
          ingress:
            enabled: true
            ingressClassName: nginx
            hosts:
              - host: ""
                paths:
                  - /vault/?(.*)
                  - /(ui(?:/.*)?)
            annotations:
              nginx.ingress.kubernetes.io/use-regex: "true"
              nginx.ingress.kubernetes.io/rewrite-target: /\$1
        ui:
          enabled: true
        EOF

        helm upgrade --install vault vault \
          --repo https://helm.releases.hashicorp.com \
          --namespace platform --create-namespace \
          --wait --timeout 15m --atomic \
          -f vault-values.yaml

  - task: AWSShellScript@1
    displayName: Deploy ArgoCD
    condition: and(succeeded(), eq('${{ parameters.action }}', 'deploy'))
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail

        cat > argocd-values.yaml <<'EOF'
        redis-ha:
          enabled: false
        controller:
          replicas: 1
        server:
          replicas: 1
          service:
            type: ClusterIP
          ingress:
            enabled: false
          extraArgs:
            - --insecure
            - --rootpath=/argocd
        repoServer:
          replicas: 1
        applicationController:
          replicas: 1
        EOF

        helm upgrade --install argocd argo-cd \
          --repo https://argoproj.github.io/argo-helm \
          --namespace platform --create-namespace \
          --wait --timeout 20m --atomic \
          -f argocd-values.yaml

        kubectl apply -f - <<'INGRESS_EOF'
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        metadata:
          name: argocd-server
          namespace: platform
          annotations:
            nginx.ingress.kubernetes.io/backend-protocol: HTTP
        spec:
          ingressClassName: nginx
          rules:
          - http:
              paths:
              - path: /argocd
                pathType: Prefix
                backend:
                  service:
                    name: argocd-server
                    port:
                      number: 80
        INGRESS_EOF

  - task: AWSShellScript@1
    displayName: Deploy SonarQube
    condition: and(succeeded(), eq('${{ parameters.action }}', 'deploy'))
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail

        cat > sonarqube-values.yaml <<'EOF'
        service:
          type: ClusterIP
        ingress:
          enabled: true
          ingressClassName: nginx
          hosts:
            - name: ""
              path: "/sonarqube"
          annotations:
            nginx.ingress.kubernetes.io/proxy-body-size: "64m"
        sonarWebContext: /sonarqube
        EOF

        helm upgrade --install sonarqube sonarqube \
          --repo https://SonarSource.github.io/helm-chart-sonarqube \
          --namespace platform --create-namespace \
          --wait --timeout 30m --atomic \
          -f sonarqube-values.yaml

  # -----------------------------
  # DESTROY MODE (add-ons only)
  # -----------------------------
  - task: AWSShellScript@1
    displayName: Destroy platform add-ons (helm uninstall + namespaces)
    condition: and(succeeded(), eq('${{ parameters.action }}', 'destroy'))
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail

        echo "Uninstalling Helm releases..."
        helm uninstall sonarqube -n platform || true
        helm uninstall argocd -n platform || true
        helm uninstall vault -n platform || true
        helm uninstall oauth2-proxy -n ingress-nginx || true
        helm uninstall ingress-nginx -n ingress-nginx || true

        echo "Deleting namespaces..."
        kubectl delete ns platform --ignore-not-found=true || true
        kubectl delete ns ingress-nginx --ignore-not-found=true || true

        echo "Done."