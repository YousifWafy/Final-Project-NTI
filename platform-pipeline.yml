name: $(Date:yyyyMMdd)$(Rev:.r)-Addons
trigger: none
pr: none

parameters:
  - name: env
    displayName: Environment
    type: string
    values: [nonprod, prod]

variables:
  TF_DIR: 'terraform'
  TF_VAR_FILE: '${{ parameters.env }}.tfvars'

  AWS_SERVICE_CONNECTION: 'yousif-aws-v2'
  AWS_REGION: 'ap-northeast-2'

  # Backend (same as infra state)
  TF_BACKEND_BUCKET: 'yousif-project-bucket'
  TF_BACKEND_KEY: '${{ parameters.env }}/terraform.tfstate'

pool:
  name: Yousif-Agent_Pool

jobs:
- job: Addons
  displayName: 'Addons Deployment (Helm, Vault, Argo, Sonar, Nginx)'
  timeoutInMinutes: 160

  steps:
  - checkout: self

  - task: TerraformInstaller@1
    displayName: Install Terraform
    inputs:
      terraformVersion: latest

  # -----------------------------
  # Terraform init (use correct backend state)
  # -----------------------------
  - task: AWSShellScript@1
    displayName: Terraform init (S3 backend + plugin cache)
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"

        echo "Terraform version:"
        terraform -version

        # Plugin cache
        export TF_PLUGIN_CACHE_DIR="${TF_PLUGIN_CACHE_DIR:-$HOME/.terraform.d/plugin-cache}"
        mkdir -p "$TF_PLUGIN_CACHE_DIR" || true
        echo "TF_PLUGIN_CACHE_DIR=$TF_PLUGIN_CACHE_DIR"

        # Ensure aws exists (macOS agent عادة موجود)
        if ! command -v aws >/dev/null 2>&1; then
          echo "##[error]aws CLI not found on agent. Install AWS CLI on the agent."
          exit 1
        fi

        echo "==> terraform init (bucket=$(TF_BACKEND_BUCKET) key=$(TF_BACKEND_KEY))"
        terraform init -reconfigure \
          -backend-config="bucket=$(TF_BACKEND_BUCKET)" \
          -backend-config="key=$(TF_BACKEND_KEY)" \
          -backend-config="region=$(AWS_REGION)" \
          -backend-config="encrypt=true"

        echo "Terraform outputs (if any):"
        terraform output || true

  # -----------------------------
  # kubeconfig (safe output read)
  # -----------------------------
  - task: AWSShellScript@1
    displayName: Configure kubeconfig
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"

        CLUSTER_NAME="$(terraform output -raw cluster_name 2>/dev/null || true)"

        if [ -z "$CLUSTER_NAME" ]; then
          echo "##[error]cluster_name output is empty. Your terraform state has no outputs (or wrong backend key)."
          echo "Backend key: $(TF_BACKEND_KEY)"
          terraform output || true
          exit 1
        fi

        if [ ${#CLUSTER_NAME} -gt 100 ] || echo "$CLUSTER_NAME" | grep -q '[[:space:]]'; then
          echo "##[error]Invalid cluster_name read from terraform output:"
          echo "$CLUSTER_NAME"
          terraform output || true
          exit 1
        fi

        echo "Updating kubeconfig for: $CLUSTER_NAME"
        aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$(AWS_REGION)"

        echo "Cluster access check:"
        kubectl get nodes --request-timeout=30s

  - task: HelmInstaller@1
    displayName: Install Helm
    inputs:
      helmVersionToInstall: 'latest'

  # -----------------------------
  # Install Nginx Ingress
  # -----------------------------
  - task: AWSShellScript@1
    displayName: Install Nginx Ingress
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail

        handle_error() {
          echo "##[error] Nginx Ingress deployment failed!"
          kubectl get all -n ingress-nginx || true
          kubectl describe pod -l app.kubernetes.io/name=ingress-nginx -n ingress-nginx || true
          kubectl logs -l app.kubernetes.io/name=ingress-nginx -n ingress-nginx --all-containers --tail=150 || true
          kubectl get events -n ingress-nginx --sort-by='.lastTimestamp' | tail -n 80 || true
        }
        trap 'handle_error' ERR

        echo "Installing Nginx Ingress Controller..."
        helm upgrade --install ingress-nginx ingress-nginx \
          --repo https://kubernetes.github.io/ingress-nginx \
          --namespace ingress-nginx --create-namespace \
          --version 4.11.3 \
          --set controller.service.type=LoadBalancer \
          --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"="nlb" \
          --set controller.admissionWebhooks.enabled=false \
          --wait --timeout 25m --rollback-on-failure --debug

        kubectl get svc -n ingress-nginx ingress-nginx-controller -o wide --request-timeout=30s

  # -----------------------------
  # Link APIGW to LB (fix: resolve LB ARN by DNSName, not by cutting name)
  # -----------------------------
  - task: AWSShellScript@1
    displayName: Link API Gateway to Load Balancer
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"

        echo "Waiting for NLB hostname..."
        NLB_DNS=""
        for i in {1..36}; do
          NLB_DNS="$(kubectl get svc -n ingress-nginx ingress-nginx-controller \
            -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' \
            --request-timeout=30s 2>/dev/null || true)"
          if [ -n "$NLB_DNS" ]; then
            echo "✅ Found NLB DNS: $NLB_DNS (after ${i} tries)"
            break
          fi
          echo "Attempt ${i}/36: NLB DNS not ready... sleeping 10s"
          sleep 10
        done

        if [ -z "$NLB_DNS" ]; then
          echo "##[warning]Load Balancer DNS not found. Skipping API Gateway update."
          kubectl get svc -n ingress-nginx ingress-nginx-controller -o wide || true
          exit 0
        fi

        echo "Resolving LoadBalancerArn by DNSName..."
        LB_ARN="$(aws elbv2 describe-load-balancers \
          --query "LoadBalancers[?DNSName=='$NLB_DNS'].LoadBalancerArn | [0]" \
          --output text)"

        if [ -z "$LB_ARN" ] || [ "$LB_ARN" = "None" ]; then
          echo "##[error]Could not resolve LB ARN for DNSName=$NLB_DNS"
          aws elbv2 describe-load-balancers --query 'LoadBalancers[].{DNS:DNSName,Arn:LoadBalancerArn}' --output table | head -n 200 || true
          exit 1
        fi

        LISTENER_ARN="$(aws elbv2 describe-listeners \
          --load-balancer-arn "$LB_ARN" \
          --query 'Listeners[0].ListenerArn' \
          --output text)"

        if [ -z "$LISTENER_ARN" ] || [ "$LISTENER_ARN" = "None" ]; then
          echo "##[error]Could not resolve Listener ARN for LB_ARN=$LB_ARN"
          aws elbv2 describe-listeners --load-balancer-arn "$LB_ARN" --output table || true
          exit 1
        fi

        echo "Running Terraform Apply to sync API Gateway..."
        terraform apply -auto-approve -var-file="$(TF_VAR_FILE)" \
          -var="nlb_dns_name=http://$NLB_DNS" \
          -var="nlb_listener_arn=$LISTENER_ARN"

  # -----------------------------
  # Export TF outputs (Cognito) safely + PKCE fallback
  # -----------------------------
  - task: AWSShellScript@1
    displayName: Export Terraform Outputs (Cognito/APIGW)
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"

        # Read outputs safely (silence warnings)
        COGNITO_CLIENT_ID="$(terraform output -raw cognito_app_client_id 2>/dev/null || true)"
        COGNITO_ISSUER_URL="$(terraform output -raw cognito_issuer_url 2>/dev/null || true)"
        APIGW_URL="$(terraform output -raw api_gateway_endpoint 2>/dev/null || true)"
        USER_POOL_ID="$(terraform output -raw cognito_user_pool_id 2>/dev/null || true)"

        if [ -z "$COGNITO_CLIENT_ID" ] || [ -z "$COGNITO_ISSUER_URL" ] || [ -z "$APIGW_URL" ] || [ -z "$USER_POOL_ID" ]; then
          echo "##[error]Missing required terraform outputs (Cognito/APIGW/UserPool). Check state/outputs."
          terraform output || true
          exit 1
        fi

        # Try fetch secret (public clients return None)
        COGNITO_CLIENT_SECRET="$(aws cognito-idp describe-user-pool-client \
          --user-pool-id "$USER_POOL_ID" \
          --client-id "$COGNITO_CLIENT_ID" \
          --query 'UserPoolClient.ClientSecret' --output text 2>/dev/null || true)"

        if [ -z "$COGNITO_CLIENT_SECRET" ] || [ "$COGNITO_CLIENT_SECRET" = "None" ]; then
          echo "##[warning]Cognito client secret not available (public client). Will use PKCE."
          COGNITO_CLIENT_SECRET=""
          USE_PKCE="true"
        else
          USE_PKCE="false"
        fi

        COOKIE_SECRET="STABLE_SECRET_V1_FOR_32_BYTE_AES"

        echo "##vso[task.setvariable variable=COGNITO_CLIENT_ID;isSecret=true]$COGNITO_CLIENT_ID"
        echo "##vso[task.setvariable variable=COGNITO_CLIENT_SECRET;isSecret=true]$COGNITO_CLIENT_SECRET"
        echo "##vso[task.setvariable variable=COGNITO_ISSUER_URL]$COGNITO_ISSUER_URL"
        echo "##vso[task.setvariable variable=APIGW_URL]$APIGW_URL"
        echo "##vso[task.setvariable variable=COOKIE_SECRET;isSecret=true]$COOKIE_SECRET"
        echo "##vso[task.setvariable variable=USE_PKCE]$USE_PKCE"

  # -----------------------------
  # OAuth2-Proxy (rollback-on-failure + diagnostics)
  # -----------------------------
  - task: AWSShellScript@1
    displayName: Deploy OAuth2-Proxy
    env:
      COGNITO_CLIENT_ID: $(COGNITO_CLIENT_ID)
      COGNITO_CLIENT_SECRET: $(COGNITO_CLIENT_SECRET)
      COOKIE_SECRET: $(COOKIE_SECRET)
      COGNITO_ISSUER_URL: $(COGNITO_ISSUER_URL)
      APIGW_URL: $(APIGW_URL)
      USE_PKCE: $(USE_PKCE)
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail

        NS="ingress-nginx"
        REL="oauth2-proxy"

        echo "Deploying oauth2-proxy..."
        echo "USE_PKCE=${USE_PKCE:-}"

        HELM_BASE="helm upgrade --install ${REL} oauth2-proxy \
          --repo https://oauth2-proxy.github.io/manifests \
          --namespace ${NS} --create-namespace \
          --wait --timeout 30m --rollback-on-failure \
          --set config.clientID=\"$COGNITO_CLIENT_ID\" \
          --set config.cookieSecret=\"$COOKIE_SECRET\" \
          --set extraArgs.provider=\"oidc\" \
          --set extraArgs.oidc-issuer-url=\"$COGNITO_ISSUER_URL\" \
          --set extraArgs.skip-provider-button=true \
          --set extraArgs.upstream=\"file:///dev/null\" \
          --set extraArgs.http-address=\"0.0.0.0:4180\" \
          --set extraArgs.redirect-url=\"${APIGW_URL}/oauth2/callback\" \
          --set ingress.enabled=true \
          --set ingress.className=\"nginx\" \
          --set ingress.path=\"/oauth2\" \
          --set ingress.hosts[0]=\"\""

        set +e
        if [ -n "${COGNITO_CLIENT_SECRET:-}" ]; then
          echo "Using client secret (confidential client)."
          eval "$HELM_BASE --set config.clientSecret=\"$COGNITO_CLIENT_SECRET\""
          RC=$?
        else
          echo "No client secret found -> enabling PKCE."
          eval "$HELM_BASE --set extraArgs.pkce=true --set extraArgs.code-challenge-method=S256"
          RC=$?
        fi
        set -e

        if [ $RC -ne 0 ]; then
          echo "##[error]oauth2-proxy helm deploy failed (rc=$RC). Collecting diagnostics..."
          kubectl get all -n "${NS}" -o wide || true
          kubectl get events -n "${NS}" --sort-by=.lastTimestamp | tail -n 60 || true
          kubectl describe deploy "${REL}" -n "${NS}" || true
          POD="$(kubectl get pods -n "${NS}" -l "app.kubernetes.io/instance=${REL}" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)"
          if [ -n "$POD" ]; then
            kubectl describe pod "$POD" -n "${NS}" || true
            kubectl logs "$POD" -n "${NS}" --tail=200 || true
            kubectl logs "$POD" -n "${NS}" --previous --tail=200 || true
          fi
          exit $RC
        fi

  # -----------------------------
  # Vault
  # -----------------------------
  - task: AWSShellScript@1
    displayName: Cleanup Vault Webhooks
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        echo "Cleaning up conflicting Vault webhooks..."
        kubectl delete MutatingWebhookConfiguration vault-agent-injector-cfg --ignore-not-found=true || true

  - task: AWSShellScript@1
    displayName: Install Vault
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail

        cat > vault-ingress-values.yaml <<'EOF'
        server:
          ha:
            enabled: false
            replicas: 1
          dataStorage:
            storageClass: gp2
            size: 1Gi
          service:
            type: ClusterIP
          ingress:
            enabled: true
            ingressClassName: nginx
            hosts:
              - host: ""
                paths:
                  - /vault/?(.*)
                  - /(ui(?:/.*)?)
            annotations:
              nginx.ingress.kubernetes.io/use-regex: "true"
              nginx.ingress.kubernetes.io/rewrite-target: /\$1
        ui:
          enabled: true
        EOF

        helm upgrade --install vault vault \
          --repo https://helm.releases.hashicorp.com \
          --namespace yousif --create-namespace \
          --wait --timeout 15m --rollback-on-failure \
          -f vault-ingress-values.yaml

  # -----------------------------
  # Argo CD
  # -----------------------------
  - task: AWSShellScript@1
    displayName: Install Argo CD
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail

        cat > argocd-ingress-values.yaml <<'EOF'
        redis-ha:
          enabled: false
        controller:
          replicas: 1
        server:
          replicas: 1
          service:
            type: ClusterIP
          ingress:
            enabled: false
          extraArgs:
            - --insecure
            - --rootpath=/argocd
        repoServer:
          replicas: 1
        applicationController:
          replicas: 1
        EOF

        helm upgrade --install argocd argo-cd \
          --repo https://argoproj.github.io/argo-helm \
          --namespace yousif --create-namespace \
          --wait --timeout 20m --rollback-on-failure \
          -f argocd-ingress-values.yaml

        kubectl apply -f - <<'INGRESS_EOF'
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        metadata:
          name: argocd-server
          namespace: yousif
          annotations:
            nginx.ingress.kubernetes.io/backend-protocol: HTTP
        spec:
          ingressClassName: nginx
          rules:
          - http:
              paths:
              - path: /argocd
                pathType: Prefix
                backend:
                  service:
                    name: argocd-server
                    port:
                      number: 80
        INGRESS_EOF

  # -----------------------------
  # SonarQube
  # -----------------------------
  - task: AWSShellScript@1
    displayName: Install SonarQube
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail

        cat > sonarqube-values.yaml <<'EOF'
        service:
          type: ClusterIP
        ingress:
          enabled: true
          ingressClassName: nginx
          hosts:
            - name: ""
              path: "/sonarqube"
          annotations:
            nginx.ingress.kubernetes.io/proxy-body-size: "64m"
        sonarWebContext: /sonarqube
        EOF

        helm upgrade --install sonarqube sonarqube \
          --repo https://SonarSource.github.io/helm-chart-sonarqube \
          --namespace yousif --create-namespace \
          --wait --timeout 30m --rollback-on-failure \
          -f sonarqube-values.yaml